{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  # of GPUs 4\n",
      "Using cuda:3 device\n",
      "Creating model from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137394/1820070400.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(config['Load_Model_Path'], map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.HDAAGT import *\n",
    "from train_test import *\n",
    "from utilz.utils import *\n",
    "import datetime\n",
    "cwd = os.getcwd()\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "with  open(\"/home/abdikhab/HDAAGT/configs/config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "cwd = os.getcwd()\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "ct = datetime.datetime.now().strftime(\"%m-%d-%H-%M\") # Current time, used for saving the log\n",
    "print(f\"Total  # of GPUs {torch.cuda.device_count()}\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "ZoneConf = Zoneconf(config['Zoneconf_path']) # This is the setting for zone configuration, you can find the image file at 'data/Zone Configuration.jpg'\n",
    "# config non-indipendent parameters\n",
    "config['NFeatures']= len(config[\"Columns_to_keep\"]) # This goes to the data preparation process\n",
    "config[\"input_size\"] = config[\"NFeatures\"] + 9 + 8 # This goes to the model, we add couple of more features to the input\n",
    "config[\"output_size\"] = len(config['xy_indx']) # The number of columns to predict\n",
    "config['device'] = device\n",
    "config['ct'] = ct\n",
    "log_code = f\"Learning rate: {config['learning_rate']}, Hidden size: {config['hidden_size']}, Batch size: {config['batch_size']}\"\n",
    "if config['verbal']:\n",
    "    for arg in config.items(): # Print the arguments to the log file\n",
    "        savelog(f\"{arg[0]} = {arg[1]}\", ct)\n",
    "config['sos'] = torch.cat((torch.tensor([10,1016,1016,7,7,7,7]) , torch.zeros(17)), dim=0).repeat(config['Nnodes'], 1).to(device)\n",
    "config['eos'] = torch.cat((torch.tensor([11,1020,1020,8,8,8,8]), torch.zeros(17)), dim=0).repeat(config['Nnodes'],1).to(device)\n",
    "\n",
    "model = HDAAGT(config).to(device) # Here we define our model\n",
    "savelog(\"Creating model from scratch\",ct)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load(config['Load_Model_Path'], map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate, hidden_size, num_layers, batch_size, ct= 1e-4, 128, 1, 128, '02-04-13-46'\n",
    "\n",
    "code = f\"LR{int(learning_rate*100000)}_HS{hidden_size}_NL{num_layers}_BS{batch_size} {ct}\"\n",
    "# Predicteddata  = torch.load(os.path.join(os.getcwd(),'Pickled', code + 'Predicteddata.pt'), weights_only=True)\n",
    "ActualTarget = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Target.pt'), weights_only=True)\n",
    "ActualScene = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Scene.pt'), weights_only=True)\n",
    "Adj_Mat_Scene = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Adj_Mat_Scene.pt'), weights_only=True)\n",
    "# Adj_Mat_Target = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Adj_Mat_Target.pt'), weights_only=True)\n",
    "# test_loss = torch.load(os.path.join(os.getcwd(),'Pickled', code + 'test_losses.pt'), weights_only=True)\n",
    "epoch_losses = torch.load(os.path.join(cwd,'Pickled', f'epoch_losses.pt'), weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "HDAAGT                                        [1, 17, 32, 2, 1024]      --\n",
       "├─Encoder_DAAG: 1-1                           [32, 17, 384]             --\n",
       "│    └─Positional_Encoding_Layer: 2-1         [32, 17, 384]             296,064\n",
       "│    │    └─ModuleList: 3-1                   --                        262,144\n",
       "│    │    └─DAAG_Layer: 3-2                   [1, 17, 32, 256]          211,456\n",
       "│    │    └─TemporalConv: 3-3                 [32, 17, 128]             20,892\n",
       "│    │    └─MultiheadAttention: 3-4           [32, 17, 384]             591,360\n",
       "│    │    └─Dropout: 3-5                      [32, 17, 384]             --\n",
       "│    └─Traffic_Encoding_Layer: 2-2            [32, 17, 384]             1,152\n",
       "│    │    └─ModuleList: 3-6                   --                        1,248\n",
       "│    │    └─LayerNorm: 3-7                    [1, 17, 32, 5]            10\n",
       "│    │    └─Linear: 3-8                       [1, 17, 32, 128]          768\n",
       "│    │    └─Linear: 3-9                       [1, 17, 32, 128]          16,512\n",
       "│    │    └─Linear: 3-10                      [1, 17, 32, 128]          16,512\n",
       "│    │    └─MultiheadAttention: 3-11          [32, 17, 384]             591,360\n",
       "│    │    └─Dropout: 3-12                     [32, 17, 384]             --\n",
       "│    │    └─FeedForwardNetwork: 3-13          [32, 17, 384]             295,680\n",
       "│    │    └─Dropout: 3-14                     [32, 17, 384]             --\n",
       "│    └─Mixed_Attention_Layer: 2-3             [32, 17, 384]             768\n",
       "│    │    └─MultiheadAttention: 3-15          [32, 17, 384]             591,360\n",
       "│    │    └─FeedForwardNetwork: 3-16          [32, 17, 384]             295,680\n",
       "│    │    └─Dropout: 3-17                     [32, 17, 384]             --\n",
       "├─Projection: 1-2                             [1, 17, 32, 2, 1024]      --\n",
       "│    └─LayerNorm: 2-4                         [32, 17, 384]             768\n",
       "│    └─Linear: 2-5                            [32, 17, 2048]            788,480\n",
       "===============================================================================================\n",
       "Total params: 3,982,214\n",
       "Trainable params: 3,982,214\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 60.54\n",
       "===============================================================================================\n",
       "Input size (MB): 0.12\n",
       "Forward/backward pass size (MB): 30.66\n",
       "Params size (MB): 7.64\n",
       "Estimated Total Size (MB): 38.42\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "from torchinfo import summary  # more flexible than torchsummary\n",
    "\n",
    "device = torch.device('cuda:3')\n",
    "def custom_summary(model, x, src_mask, adj_mat):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x, src_mask, adj_mat)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Dummy input example (adjust shapes accordingly)\n",
    "x = attach_sos_eos(ActualScene[:1], config['sos'], config['eos'])\n",
    "adj_mat = torch.cat((torch.ones_like(Adj_Mat_Scene[:1,:1]), Adj_Mat_Scene[:1], torch.ones_like(Adj_Mat_Scene[:1,:1])), dim=1)\n",
    "src_mask = create_src_mask(x)\n",
    "# custom_summary(model, x, src_mask, adj_mat)\n",
    "summary(model, input_size=[x.size(), src_mask.size(), adj_mat.size()], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/HDAAGT_graph.png'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "model.eval()\n",
    "output = model(x, src_mask, adj_mat)\n",
    "\n",
    "# Create graph\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('models/HDAAGT_graph', cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([390, 17, 32, 2]) torch.Size([390, 17, 32, 2])\n",
      "tensor(2855.8521, device='cuda:3') tensor(2855.8521, device='cuda:3')\n",
      "tensor(14.9209, device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch = 32\n",
    "Predicteddata = []\n",
    "for n in range(0, ActualScene.size(0), batch):\n",
    "    x, src_mask,adj_mat, target = prep_model_input(ActualScene[n:n+batch], Adj_Mat_Scene[n:n+batch], ActualTarget[n:n+batch], config['sos'], config['eos'], config)\n",
    "    Predicteddata += model(x, src_mask, adj_mat)\n",
    "\n",
    "Predicteddata = torch.stack(Predicteddata)\n",
    "a = Predicteddata[:,:].softmax(dim=-1)\n",
    "top_values, top_indices = torch.topk(a, k = 5, dim=-1)\n",
    "pred_tar0 = top_indices[:,:,:,:,0]\n",
    "pred_tar = (top_indices*top_values).sum(-1)/top_values.sum(-1)\n",
    "print(pred_tar0.shape, pred_tar.shape)\n",
    "true_loss = F.mse_loss(top_indices[:,1:-1,:,:,0], ActualTarget[:,:,:,1:3])\n",
    "lvl2_loss = F.mse_loss(pred_tar0[:,1:-1], ActualTarget[:,:,:,1:3])\n",
    "print(true_loss, lvl2_loss)\n",
    "error = torch.sqrt(torch.pow((pred_tar0[:,1:-1]- ActualTarget[:,:,:,1:3]),2).sum(-1)).mean()\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'angle' can not be treated as a double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m180\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39matan((scene[i,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m-\u001b[39mscene[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m/\u001b[39m(scene[i,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m scene[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpi  \u001b[38;5;66;03m# degrees\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Rotate icon (preserving alpha channel)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetRotationMatrix2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m rotated_icon \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpAffine(icon, M, (w, h), flags\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_LINEAR, borderMode\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mBORDER_CONSTANT, borderValue\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Split rotated icon into BGR and alpha\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'angle' can not be treated as a double"
     ]
    }
   ],
   "source": [
    "\n",
    "img0 = cv2.imread(\"data/no-car.JPG\")\n",
    "path = \"Processed/results\"\n",
    "icon = cv2.imread(\"data/icon.png\", cv2.IMREAD_UNCHANGED)  # Includes alpha\n",
    "icon = cv2.resize(icon, (50, 50))\n",
    "\n",
    "# Rotation parameters\n",
    "(h, w) = icon.shape[:2]\n",
    "center = (w // 2, h // 2)\n",
    "\n",
    "for m in range(398):\n",
    "    img = img0.copy()\n",
    "    scene = ActualScene[m]\n",
    "    tar = ActualTarget[m,-1]\n",
    "    pred = pred_tar0[m,-2]\n",
    "    for i in range(1, 15):\n",
    "        angle = 180*torch.atan((scene[i,0,2]-scene[i-1,0,2])/(scene[i,0,1] - scene[i-1,0,1]))/torch.pi  # degrees\n",
    "        # Rotate icon (preserving alpha channel)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        rotated_icon = cv2.warpAffine(icon, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0,0))\n",
    "        # Split rotated icon into BGR and alpha\n",
    "        icon_rgb = rotated_icon[:, :, :3]\n",
    "        alpha = rotated_icon[:, :, 3] / 255.0\n",
    "        # Set position to place icon\n",
    "        x_offset, y_offset = 10, 10\n",
    "        # Define ROI on the background\n",
    "        roi = background[y_offset:y_offset+h, x_offset:x_offset+w]\n",
    "\n",
    "        # Blend icon into background\n",
    "        for c in range(3):\n",
    "            roi[:, :, c] = roi[:, :, c] * (1 - alpha) + icon_rgb[:, :, c] * alpha\n",
    "\n",
    "        # Replace the region in the original image\n",
    "        background[y_offset:y_offset+h, x_offset:x_offset+w] = roi\n",
    "\n",
    "        egox, egoy = scene[:, i, 0], scene[:,i, 1]\n",
    "        targetx, targety = tar[i, 0], tar[i, 1]\n",
    "        # if dir != 0:\n",
    "        for n in range(int(egox.size(0)-1)):\n",
    "            if egox[n] > 10 and egox[n+1] > 10:\n",
    "                cv2.line(img, (int(egox[n]), int(egoy[n])),(int(egox[n+1]), int(egoy[n+1])), color = rand_color[i], thickness= 4)\n",
    "                # cv2.circle(img, (int(egox[n]), int(egoy[n])), 5, rand_color[i], -1)\n",
    "        cv2.circle(img, (int(tar[i, 0]), int(tar[i, 1])), 10, rand_color[i], -1)\n",
    "        cv2.rectangle(img, (int(pred[i, 0]), int(pred[i, 1])), (int(pred[i, 0]+15), int(pred[i, 1]+15)), rand_color[i], -1)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(path, f\"pred{m}.jpg\"), img)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "print(\"Images created successfully\")\n",
    "\n",
    "\n",
    "# Load background and icon\n",
    "background = cv2.imread(\"background.jpg\")\n",
    "\n",
    "\n",
    "# Resize if needed\n",
    "icon = cv2.resize(icon, (50, 50))\n",
    "\n",
    "# Rotation parameters\n",
    "angle = 45  # degrees\n",
    "(h, w) = icon.shape[:2]\n",
    "center = (w // 2, h // 2)\n",
    "\n",
    "\n",
    "# Show result\n",
    "cv2.imshow(\"Rotated Icon on Background\", background)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_color = [\n",
    "    (0, 0, 0),        # Black\n",
    "    (0, 0, 255),      # Red\n",
    "    (0, 255, 0),      # Green\n",
    "    (255, 0, 0),      # Blue\n",
    "    (0, 255, 255),    # Yellow\n",
    "    (255, 255, 0),    # Cyan\n",
    "    (255, 0, 255),    # Magenta\n",
    "    (128, 0, 128),    # Purple\n",
    "    (0, 165, 255),    # Orange\n",
    "    (130, 0, 75),     # Indigo\n",
    "    (128, 128, 0),    # Teal\n",
    "    (0, 0, 128),      # Maroon\n",
    "    (128, 0, 0),       # Navy\n",
    "    (128, 128, 128),  # Gray\n",
    "    (192, 192, 192),  # Silver\n",
    "    (255, 255, 255),   # White\n",
    "    (255, 165, 0),     # Brown\n",
    "    (0, 128, 0),       # Olive\n",
    "    (0, 128, 128),     # Aqua\n",
    "    (128, 0, 128),     # Fuchsia\n",
    "    (128, 128, 0),     # Lime\n",
    "# Some more colors\n",
    "    (50, 50, 50),      # Dark Gray\n",
    "    (60, 120, 120),    # Sea Green\n",
    "    (120, 30, 60),     # Crimson\n",
    "    (100, 20, 120),    # Violet\n",
    "    (10, 32, 64),       # Dark Blue\n",
    "    (64,10, 32),        # Dark Red\n",
    "    (10, 64, 32),       # Dark Green\n",
    "    (16, 32, 64),       # Dark Aqua\n",
    "    (32, 64, 16),       # Dark Lime\n",
    "    (64, 32, 16),       # Dark Orange\n",
    "    (64, 16, 32),       # Dark Pink\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import re\n",
    "from shapely.geometry import Point, Polygon\n",
    "def Zoneconf1():\n",
    "    ZoneConf = []\n",
    "    with open('utilz/ZoneConf.yaml') as file:\n",
    "        ZonesYML = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        #convert the string values to float\n",
    "        for _, v in ZonesYML.items():\n",
    "            lst = []\n",
    "            for _, p  in v.items():    \n",
    "                for x in p[0]:\n",
    "                    b = re.split(r'[,()]',p[0][x])\n",
    "                    lst.append((float(b[1]), float(b[2])))\n",
    "            ZoneConf.append(lst)\n",
    "    return ZoneConf\n",
    "\n",
    "def zonefinder(BB, Zones):\n",
    "    for i, zone in enumerate(Zones):\n",
    "        Poly = Polygon(zone)\n",
    "        if Poly.contains(Point(BB[0], BB[1])):\n",
    "            return i\n",
    "    return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoneconf = Zoneconf1()\n",
    "zones = torch.empty(0,2)\n",
    "zonesw0 = torch.empty(0,2)\n",
    "err = 0\n",
    "for m in range(440):\n",
    "    tar = target[m,-1].to('cpu')\n",
    "    pred = pred_tar[m,-4:-1].mean(0).to('cpu')\n",
    "    for i in range(32):\n",
    "        zt = zonefinder(tar[i],zoneconf)\n",
    "        zp = zonefinder(pred[i], zoneconf)\n",
    "        if zp == 100 and zt != 100:\n",
    "            # print(pred[i], tar[i], zp, zt)\n",
    "            err +=1\n",
    "        else:\n",
    "            zonesw0 = torch.cat((zonesw0,torch.tensor([[zp,zt]])), dim=0)\n",
    "            if zt !=100:\n",
    "                zones = torch.cat((zones,torch.tensor([[zp,zt]])), dim=0)\n",
    "\n",
    "print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (zones[:,0]==zones[:,1]).sum()\n",
    "b = zones.size(0)\n",
    "print(\"Accuracy without zone 100 is \",a, b, a/b )\n",
    "a = (zonesw0[:,0]==zonesw0[:,1]).sum()\n",
    "b = zonesw0.size(0)\n",
    "print(\"Accuracy with zone 100 is \",a, b, a/b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(zones[:,1], zones[:,0])\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm )\n",
    "disp = disp.plot(cmap=plt.cm.YlGn,values_format='g')\n",
    "# disp.plot()\n",
    "plt.xlabel('Predicted Zones')\n",
    "plt.ylabel('True Zones')\n",
    "plt.title('Confusion Matrix')\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = disp.plot(cmap=plt.cm.YlGnBu,values_format='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img0 = cv2.imread(\"/home/abdikhab/Traj_Pred/RawData/no-car.JPG\")\n",
    "path = \"/home/abdikhab/New_Idea_Traj_Pred/results\"\n",
    "for m in range(440):\n",
    "    img = img0.copy()\n",
    "    scene = scene[m]\n",
    "    tar = target[m,-1]\n",
    "    pred = pred_tar0[m,-2]\n",
    "    for i in range(32):\n",
    "        \n",
    "        egox, egoy = scene[:, i, 0], scene[:,i, 1]\n",
    "        targetx, targety = tar[i, 0], tar[i, 1]\n",
    "        # if dir != 0:\n",
    "        for n in range(int(egox.size(0)-1)):\n",
    "            if egox[n] > 10 and egox[n+1] > 10:\n",
    "                cv2.line(img, (int(egox[n]), int(egoy[n])),(int(egox[n+1]), int(egoy[n+1])), color = rand_color[i], thickness= 4)\n",
    "                # cv2.circle(img, (int(egox[n]), int(egoy[n])), 5, rand_color[i], -1)\n",
    "        cv2.circle(img, (int(tar[i, 0]), int(tar[i, 1])), 10, rand_color[i], -1)\n",
    "        cv2.rectangle(img, (int(pred[i, 0]), int(pred[i, 1])), (int(pred[i, 0]+15), int(pred[i, 1]+15)), rand_color[i], -1)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(path, f\"pred{m}.jpg\"), img)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "print(\"Images created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss\n",
    "trainloss = []\n",
    "testloss = []\n",
    "epochs = []\n",
    "for obj in test_loss:\n",
    "    trainloss.append(epoch_losses[obj[1]])\n",
    "    testloss.append(obj[0])\n",
    "    epochs.append(obj[1])\n",
    "plt.plot(epochs, trainloss)\n",
    "plt.title('Epoch Losses during batch Training')\n",
    "plt.plot(epochs, testloss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train Loss', 'Test Loss'])\n",
    "plt.ylim(0, 100000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now, we want to predict the future sequence using the best model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilz.utils import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Best_Model=torch.load(os.path.join(os.getcwd(),'Processed', code + 'Bestmodel.pth'))\n",
    "# Best_Model_dict = Best_Model.state_dict()\n",
    "\n",
    "# Model = GGAT(args)\n",
    "# Model.load_state_dict(Best_Model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, Scene, Adj_mat, h0, Nseq):\n",
    "    # savelog(\"Starting testing phase\", ct)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "        Out, Gruh0 = model(Scene, Adj_mat, h0, Nseq)\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        inference_time = start_event.elapsed_time(end_event)\n",
    "    return Out, Gruh0, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach(NScene, Scenelet, Zonecft):\n",
    "    x,y = NScene[:,:,0], NScene[:,:,1]\n",
    "\n",
    "    dx = torch.diff(x, n=1, append=x[-1:,:], dim = 0).unsqueeze(2)\n",
    "    dy = torch.diff(y, n=1, append=y[-1:,:], dim = 0).unsqueeze(2)\n",
    "    heading = torch.atan2(dy, dx)\n",
    "    xc = x - 512\n",
    "    yc = y - 512\n",
    "    Rc = torch.sqrt(xc**2 + yc**2).unsqueeze(2)\n",
    "    SinX = torch.sin(xc/512).unsqueeze(2)\n",
    "    CosY = torch.cos(yc/512).unsqueeze(2)\n",
    "    SinY = torch.sin(yc/512).unsqueeze(2)\n",
    "    CosX = torch.cos(xc/512).unsqueeze(2)\n",
    "    Sin2X = torch.sin(2*xc/512).unsqueeze(2)\n",
    "    Cos2X = torch.cos(2*xc/512).unsqueeze(2)\n",
    "    Sin2Y = torch.sin(2*yc/512).unsqueeze(2)\n",
    "    Cos2Y = torch.cos(2*yc/512).unsqueeze(2)\n",
    "\n",
    "\n",
    "    NScene = torch.cat((NScene, Scenelet, dx, dy, heading, xc.unsqueeze(2),yc.unsqueeze(2), Rc,\n",
    "                            SinX, CosX, SinY, CosY,\n",
    "                            Sin2X, Cos2X, Sin2Y, Cos2Y), dim=2)\n",
    "    for i in range(NScene.size(1)):\n",
    "        NScene[:,i,9] = torch.tensor(zonefinder(NScene[0,i,:2].cpu().numpy(), Zonecft), device=NScene.device)\n",
    "    return NScene\n",
    "\n",
    "def Zoneconf():\n",
    "    ZoneConf = []\n",
    "    with open('utilz/ZoneConf.yaml') as file:\n",
    "        ZonesYML = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        #convert the string values to float\n",
    "        for _, v in ZonesYML.items():\n",
    "            lst = []\n",
    "            for _, p  in v.items():    \n",
    "                for x in p[0]:\n",
    "                    b = re.split(r'[,()]',p[0][x])\n",
    "                    lst.append((float(b[1]), float(b[2])))\n",
    "            ZoneConf.append(lst)\n",
    "    return ZoneConf\n",
    "\n",
    "def zonefinder(BB, Zones):\n",
    "    for i, zone in enumerate(Zones):\n",
    "        Poly = Polygon(zone)\n",
    "        if Poly.contains(Point(BB[0], BB[1])):\n",
    "            return i\n",
    "    return 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for the case only one time step is going to be passed through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene_eval(Best_Model, Scene, Adj_Mat,Target, future, Zonecft):\n",
    "    Pred = []\n",
    "    total_time = 0\n",
    "    NScene, GRUh0, inference_time = eval(Best_Model, Scene[:,-20:,:,:], Adj_Mat[:,:20,:32,:32], None, False)\n",
    "    Pred.append(NScene)\n",
    "    NScene = attach(NScene, Target[:,0,:,2:10], Zonecft).unsqueeze(0)\n",
    "    # NScene = torch.cat((Scene[:,-19:,:,:],NScene), dim=1)\n",
    "    \n",
    "    for f in range(future-1):\n",
    "        # NScene1, GRUh0, inference_time = eval(Best_Model, NScene, Adj_Mat[:,f+1:f+21,:32,:32], GRUh0[:,0,:].unsqueeze(1), False)\n",
    "        NScene1, GRUh0, inference_time = eval(Best_Model, NScene, Adj_Mat[:,f+1,:32,:32].unsqueeze(1), GRUh0, False)\n",
    "        Pred.append(NScene1)\n",
    "        NScene1 = attach(NScene1, Target[:,f+1,:,2:10], Zonecft).unsqueeze(0)\n",
    "        # NScene = torch.cat((NScene[:,-19:,:,:], NScene1), dim=1)\n",
    "        total_time += inference_time\n",
    "\n",
    "    Pred = torch.stack(Pred, dim=1).to(torch.int)\n",
    "    tag = Target[0,:,:,:2].unsqueeze(0) != 0\n",
    "    Pred = Pred*(tag.to(torch.int))\n",
    "\n",
    "    loss = F.mse_loss(Pred, Target[0,:,:,:2].unsqueeze(0))\n",
    "    msePtime = torch.sqrt(torch.sum((Pred - Target[0,:,:,:2].unsqueeze(0)) ** 2, dim = -1)).sum(dim = -1)/32\n",
    "    msePuser = torch.sqrt(torch.sum((Pred - Target[0,:,:,:2].unsqueeze(0)) ** 2, dim = -1)).sum(dim = 1)/future\n",
    "\n",
    "    # print(f\"loss: {loss.item()} and inference time: {total_time} ms\")\n",
    "    # print(f\"mse per time step: {msePtime}\")\n",
    "    # print(f\"mse per user: {msePuser}\")\n",
    "    return Pred, loss, msePtime.squeeze(0), msePuser.squeeze(0), total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene_eval20(Best_Model, Scene, Adj_Mat,Target, future, Zonecft):\n",
    "    Pred = []\n",
    "    total_time = 0\n",
    "    NScene, GRUh0, inference_time = eval(Best_Model, Scene[:,-20:,:,:], Adj_Mat[:,:20,:32,:32], None, False)\n",
    "    Pred.append(NScene)\n",
    "    NScene = attach(NScene, Target[:,0,:,2:10], Zonecft).unsqueeze(0)\n",
    "    NScene = torch.cat((Scene[:,-19:,:,:], NScene), dim=1)\n",
    "    \n",
    "    for f in range(future-1):\n",
    "        NScene1, GRUh0, inference_time = eval(Best_Model, NScene, Adj_Mat[:,f+1:f+21,:32,:32], GRUh0[:,0,:].unsqueeze(1), False)\n",
    "        Pred.append(NScene1)\n",
    "        NScene1 = attach(NScene1, Target[:,f+1,:,2:10], Zonecft).unsqueeze(0)\n",
    "        NScene = torch.cat((NScene[:,-19:,:,:], NScene1), dim=1)\n",
    "        total_time += inference_time\n",
    "\n",
    "    Pred = torch.stack(Pred, dim=1)\n",
    "    tag = Target[0,:,:,:2].unsqueeze(0) != 0\n",
    "    Pred = Pred*(tag.to(torch.int))\n",
    "\n",
    "    loss = F.mse_loss(Pred, Target[0,:,:,:2].unsqueeze(0))\n",
    "    msePtime = torch.sqrt(torch.sum((Pred - Target[0,:,:,:2].unsqueeze(0)) ** 2, dim = -1)).sum(dim = -1)/32\n",
    "    msePuser = torch.sqrt(torch.sum((Pred - Target[0,:,:,:2].unsqueeze(0)) ** 2, dim = -1)).sum(dim = 1)/future\n",
    "\n",
    "    # print(f\"loss: {loss.item()} and inference time: {total_time} ms\")\n",
    "    # print(f\"mse per time step: {msePtime}\")\n",
    "    # print(f\"mse per user: {msePuser}\")\n",
    "    return Pred, loss, msePtime.squeeze(0), msePuser.squeeze(0), total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = 30\n",
    "Prediction = []\n",
    "totalmsePtime = torch.zeros(30, device=Adj_Mat_Scene.device)\n",
    "totalmsePuser = torch.zeros(32, device=Adj_Mat_Scene.device)\n",
    "total_loss = 0\n",
    "\n",
    "total_time = 0\n",
    "Adj_Mat = torch.cat((Adj_Mat_Scene, Adj_Mat_Target), dim=1).unsqueeze(0)\n",
    "Zonecft = Zoneconf()\n",
    "for n in range(250):\n",
    "    Scene = ActualScene[n,1:,:32,:].unsqueeze(0)\n",
    "    Target = ActualTarget[n,:,:32].unsqueeze(0)\n",
    "    adj_mat = Adj_Mat[:,n,:,:32,:32]\n",
    "    Pred, loss, msePtime, msePuser, elapsed_time = scene_eval(Best_Model, Scene, adj_mat, Target, future, Zonecft)\n",
    "    totalmsePtime = totalmsePtime+ msePtime\n",
    "    totalmsePuser = totalmsePuser+ msePuser\n",
    "    total_loss += loss\n",
    "    total_time += elapsed_time\n",
    "    Prediction.append(Pred)\n",
    "\n",
    "totalmsePtime = totalmsePtime/250\n",
    "totalmsePuser = totalmsePuser/250\n",
    "total_loss = total_loss/250\n",
    "Prediction = torch.cat(Prediction, dim=0)\n",
    "print(f\"Total loss: {total_loss.item()}\")\n",
    "print(f\"Total mse per time step: {totalmsePtime}\")\n",
    "print(f\"Total mse per user: {totalmsePuser}\")\n",
    "print(f\"Total inference time: {total_time/250} ms\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scene = 50\n",
    "obj = 60\n",
    "compare = torch.cat((Prediction[n_scene,:,obj], ActualTarget[n_scene,:,obj,:2]), dim=-1)\n",
    "print(compare.to(torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preddata = Predicteddata.cpu()\n",
    "acttarget = ActualTarget.cpu()\n",
    "actscene = ActualScene.cpu()\n",
    "\n",
    "# 41 different colors codes\n",
    "color = torch.randint(0, 255, (41, 3), dtype=torch.uint8)\n",
    "print(actscene.shape, acttarget.shape, preddata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 2 # test sample number\n",
    "\n",
    "i = 0\n",
    "x = preddata[n][:,:32, 0]\n",
    "y = preddata[n][:,:32, 1]\n",
    "xs = actscene[n][:20,:32, 0]\n",
    "ys = actscene[n][:20,:32, 1]\n",
    "xr = acttarget[n][:20,:32, 10]\n",
    "yr = acttarget[n][:20,:32, 11]\n",
    "# Pliotting the predicted data\n",
    "for keys in colors.keys():\n",
    "    if i < 32:\n",
    "        plt.scatter(xs[:,i], ys[:,i], color = colors[keys])\n",
    "        plt.scatter(xr[:,i], yr[:,i], color = colors[keys])\n",
    "        plt.scatter(x[:,i], y[:,i], color = colors[keys])\n",
    "        # plt.scatter(xr[:,i], yr[:,i], color = 'red')\n",
    "        # plt.scatter(x[:,i], y[:,i], color = 'black')\n",
    "        plt.title('Predicted Data')\n",
    "        i += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image and plot the ego and target on the image\n",
    "import matplotlib.pyplot as plt\n",
    "color = torch.randint(0, 255, (32, 3), dtype=torch.uint8)\n",
    "img = cv2.imread('RawData/1.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "n = 30\n",
    "\n",
    "\n",
    "xs = actscene[n][:,:32, 0]\n",
    "ys = actscene[n][:,:32, 1]\n",
    "xr = acttarget[n][:,:32, 0]\n",
    "yr = acttarget[n][:,:32, 1]\n",
    "for m in range(32):\n",
    "    # add points to the coordinates on pred, grth and grth_target\n",
    "    for i in range(20):\n",
    "        cv2.circle(img, (int(xs[i,m]), int(ys[i,m])), 2, color[m].tolist(), -1)\n",
    "        cv2.circle(img, (int(xr[i,m]), int(yr[i,m])), 4, color[m].tolist(), -1)\n",
    "        # cv2.circle(img, (int(grth_target[i,0]), int(grth_target[i,1])), 4, (255,0,0), -1)\n",
    "    for i in range(20, 30):\n",
    "        cv2.circle(img, (int(xr[i,m]), int(yr[i,m])), 4, color[m].tolist(), -1)\n",
    "#plot polygon on the image\n",
    "# for i, zone in enumerate(ZoneConf):\n",
    "#     cv2.polylines(img, [np.array(zone).astype(np.int32)], isClosed=True, color=(0,0,0), thickness=1)\n",
    "plt.imshow(img)\n",
    "plt.legend(['Predicted', 'Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "'aliceblue':            '#F0F8FF',\n",
    "'antiquewhite':         '#FAEBD7',\n",
    "'aqua':                 '#00FFFF',\n",
    "'aquamarine':           '#7FFFD4',\n",
    "'azure':                '#F0FFFF',\n",
    "'beige':                '#F5F5DC',\n",
    "'bisque':               '#FFE4C4',\n",
    "'black':                '#000000',\n",
    "'blanchedalmond':       '#FFEBCD',\n",
    "'blue':                 '#0000FF',\n",
    "'blueviolet':           '#8A2BE2',\n",
    "'brown':                '#A52A2A',\n",
    "'burlywood':            '#DEB887',\n",
    "'cadetblue':            '#5F9EA0',\n",
    "'chartreuse':           '#7FFF00',\n",
    "'chocolate':            '#D2691E',\n",
    "'coral':                '#FF7F50',\n",
    "'cornflowerblue':       '#6495ED',\n",
    "'cornsilk':             '#FFF8DC',\n",
    "'crimson':              '#DC143C',\n",
    "'cyan':                 '#00FFFF',\n",
    "'darkblue':             '#00008B',\n",
    "'darkcyan':             '#008B8B',\n",
    "'darkgoldenrod':        '#B8860B',\n",
    "'darkgray':             '#A9A9A9',\n",
    "'darkgreen':            '#006400',\n",
    "'darkkhaki':            '#BDB76B',\n",
    "'darkmagenta':          '#8B008B',\n",
    "'darkolivegreen':       '#556B2F',\n",
    "'darkorange':           '#FF8C00',\n",
    "'darkorchid':           '#9932CC',\n",
    "'darkred':              '#8B0000',\n",
    "'darksalmon':           '#E9967A',\n",
    "'darkseagreen':         '#8FBC8F',\n",
    "'darkslateblue':        '#483D8B',\n",
    "'darkslategray':        '#2F4F4F',\n",
    "'darkturquoise':        '#00CED1',\n",
    "'darkviolet':           '#9400D3',\n",
    "'deeppink':             '#FF1493',\n",
    "'deepskyblue':          '#00BFFF',\n",
    "'dimgray':              '#696969',\n",
    "'dodgerblue':           '#1E90FF',\n",
    "'firebrick':            '#B22222',\n",
    "'floralwhite':          '#FFFAF0',\n",
    "'forestgreen':          '#228B22',\n",
    "'fuchsia':              '#FF00FF',\n",
    "'gainsboro':            '#DCDCDC',\n",
    "'ghostwhite':           '#F8F8FF',\n",
    "'gold':                 '#FFD700',\n",
    "'goldenrod':            '#DAA520',\n",
    "'gray':                 '#808080',\n",
    "'green':                '#008000',\n",
    "'greenyellow':          '#ADFF2F',\n",
    "'honeydew':             '#F0FFF0',\n",
    "'hotpink':              '#FF69B4',\n",
    "'indianred':            '#CD5C5C',\n",
    "'indigo':               '#4B0082',\n",
    "'ivory':                '#FFFFF0',\n",
    "'khaki':                '#F0E68C',\n",
    "'lavender':             '#E6E6FA',\n",
    "'lavenderblush':        '#FFF0F5',\n",
    "'lawngreen':            '#7CFC00',\n",
    "'lemonchiffon':         '#FFFACD',\n",
    "'lightblue':            '#ADD8E6',\n",
    "'lightcoral':           '#F08080',\n",
    "'lightcyan':            '#E0FFFF',\n",
    "'lightgoldenrodyellow': '#FAFAD2',\n",
    "'lightgreen':           '#90EE90',\n",
    "'lightgray':            '#D3D3D3',\n",
    "'lightpink':            '#FFB6C1',\n",
    "'lightsalmon':          '#FFA07A',\n",
    "'lightseagreen':        '#20B2AA',\n",
    "'lightskyblue':         '#87CEFA',\n",
    "'lightslategray':       '#778899',\n",
    "'lightsteelblue':       '#B0C4DE',\n",
    "'lightyellow':          '#FFFFE0',\n",
    "'lime':                 '#00FF00',\n",
    "'limegreen':            '#32CD32',\n",
    "'linen':                '#FAF0E6',\n",
    "'magenta':              '#FF00FF',\n",
    "'maroon':               '#800000',\n",
    "'mediumaquamarine':     '#66CDAA',\n",
    "'mediumblue':           '#0000CD',\n",
    "'mediumorchid':         '#BA55D3',\n",
    "'mediumpurple':         '#9370DB',\n",
    "'mediumseagreen':       '#3CB371',\n",
    "'mediumslateblue':      '#7B68EE',\n",
    "'mediumspringgreen':    '#00FA9A',\n",
    "'mediumturquoise':      '#48D1CC',\n",
    "'mediumvioletred':      '#C71585',\n",
    "'midnightblue':         '#191970',\n",
    "'mintcream':            '#F5FFFA',\n",
    "'mistyrose':            '#FFE4E1',\n",
    "'moccasin':             '#FFE4B5',\n",
    "'navajowhite':          '#FFDEAD',\n",
    "'navy':                 '#000080',\n",
    "'oldlace':              '#FDF5E6',\n",
    "'olive':                '#808000',\n",
    "'olivedrab':            '#6B8E23',\n",
    "'orange':               '#FFA500',\n",
    "'orangered':            '#FF4500',\n",
    "'orchid':               '#DA70D6',\n",
    "'palegoldenrod':        '#EEE8AA',\n",
    "'palegreen':            '#98FB98',\n",
    "'paleturquoise':        '#AFEEEE',\n",
    "'palevioletred':        '#DB7093',\n",
    "'papayawhip':           '#FFEFD5',\n",
    "'peachpuff':            '#FFDAB9',\n",
    "'peru':                 '#CD853F',\n",
    "'pink':                 '#FFC0CB',\n",
    "'plum':                 '#DDA0DD',\n",
    "'powderblue':           '#B0E0E6',\n",
    "'purple':               '#800080',\n",
    "'red':                  '#FF0000',\n",
    "'rosybrown':            '#BC8F8F',\n",
    "'royalblue':            '#4169E1',\n",
    "'saddlebrown':          '#8B4513',\n",
    "'salmon':               '#FA8072',\n",
    "'sandybrown':           '#FAA460',\n",
    "'seagreen':             '#2E8B57',\n",
    "'seashell':             '#FFF5EE',\n",
    "'sienna':               '#A0522D',\n",
    "'silver':               '#C0C0C0',\n",
    "'skyblue':              '#87CEEB',\n",
    "'slateblue':            '#6A5ACD',\n",
    "'slategray':            '#708090',\n",
    "'snow':                 '#FFFAFA',\n",
    "'springgreen':          '#00FF7F',\n",
    "'steelblue':            '#4682B4',\n",
    "'tan':                  '#D2B48C',\n",
    "'teal':                 '#008080',\n",
    "'thistle':              '#D8BFD8',\n",
    "'tomato':               '#FF6347',\n",
    "'turquoise':            '#40E0D0',\n",
    "'violet':               '#EE82EE',\n",
    "'wheat':                '#F5DEB3',\n",
    "'white':                '#FFFFFF',\n",
    "'whitesmoke':           '#F5F5F5',\n",
    "'yellow':               '#FFFF00',\n",
    "'yellowgreen':          '#9ACD32'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "cwd = os.getcwd()\n",
    "csvpath = os.path.join(cwd,'Processed','Trj20240506T2121.csv')\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "# # Hyperparameters\n",
    "hidden_size, num_layersGAT, num_layersGRU, expansion,  n_heads = 128, 2,6, 2, 8\n",
    "\n",
    "learning_rate, schd_stepzise, gamma, epochs, batch_size, patience_limit, clip= 2e-4, 20, 0.5, 300, 128, 40, 1\n",
    "\n",
    "Headers = ['Frame', 'ID', 'BBx', 'BBy','W', 'L' , 'Cls','Tr1', 'Tr2', 'Tr3', 'Tr4', 'Zone', 'Xreal', 'Yreal']\n",
    "Columns_to_keep = [2,3,4,5,6,7,8,9,10,11] #['BBx', 'BBy','W', 'L' , 'Cls','Tr1', 'Tr2', 'Tr3', 'Tr4', 'Zone']\n",
    "#  ['Vx', 'Vy', 'heading',xc,yc, Rc,Rc, SinX,CosX,SinY,CosY, Sin2X,Cos2X, Sin2Y, Cos2Y, Sin3X, Cos3X, Sin3Y, Cos3Y, Sin4X, Cos4X, Sin4Y, Cos4Y]\n",
    "Columns_to_Predict = [0,1] #['BBx', 'BBy','Xreal', 'Yreal','Vx', 'Vy']\n",
    "xyid = [0, 1] # the index of x and y  of the Columns_to_Keep in the columns for speed calculation\n",
    "TrfL_Columns = [7,8,9,10]\n",
    "NFeatures = len(Columns_to_keep)\n",
    "input_size = NFeatures + 6 + 8\n",
    "output_size = len(Columns_to_Predict)\n",
    "Nusers, NZones = 32, 9\n",
    "Nnodes, NTrfL, sl, sw, sn  = Nusers+NZones, 4, 20, 1, 5\n",
    "future = 30\n",
    "Centre = [512,512]\n",
    "\n",
    "sos = torch.tensor([1022,1022], device=device)\n",
    "eos = torch.tensor([1021,1021], device=device)\n",
    "\n",
    "generate_data = False #  Add class by loading the CSV file\n",
    "# generate_data = True #  Add class by loading the CSV file\n",
    "loadData = not generate_data      # load the class from the saved file\n",
    "Train = True # It's for training the model with the prepared data\n",
    "test_in_epoch = True # It's for testing the model in each epoch\n",
    "model_from_scratch = True # It's for creating model from scratch\n",
    "load_the_model = not model_from_scratch # It's for loading model\n",
    "Seed = loadData # If true, it will use the predefined seed to load the indices\n",
    "only_test = False # It's for testing the model only\n",
    "concat = False\n",
    "\n",
    "\n",
    "\n",
    "LR = [learning_rate] #, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0]\n",
    "# LR = [1e-2]\n",
    "HS = [128] #, 32, 64, 128, 256, 512, 1024]\n",
    "NL = [num_layersGAT] #, 2, 3, 4]\n",
    "BS = [batch_size] #, 32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "csvpath ='/home/abdikhab/New_Idea_Traj_Pred/RawData/TrfZonXYCam.csv'\n",
    "Zoneconf_path ='/home/abdikhab/New_Idea_Traj_Pred/utilz/ZoneConf.yaml'\n",
    "dataset_path = os.path.join(os.getcwd(), 'Pickled')\n",
    "ct = datetime.datetime.now().strftime(\"%m-%d-%H-%M\")\n",
    "print(f\"Total  # of GPUs {torch.cuda.device_count()}\")\n",
    "# spread the model to multiple GPUs\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "# Use argparse to get the parameters\n",
    "parser = argparse.ArgumentParser(description='Trajectory Prediction')\n",
    "parser.add_argument('--ct', type=str, default=ct, help='Current time')\n",
    "parser.add_argument('--Zonepath', type=str, default=Zoneconf_path, help='Zone Conf path')\n",
    "parser.add_argument('--Nfeatures', type=int, default=NFeatures, help='Number of features')\n",
    "parser.add_argument('--Nnodes', type=int, default=Nnodes, help='Number of nodes')\n",
    "parser.add_argument('--NZones', type=int, default=NZones, help='Number of zones')\n",
    "parser.add_argument('--NTrfL', type=int, default=NTrfL, help='Number of traffic lights')\n",
    "parser.add_argument('--sl', type=int, default=sl, help='Sequence length')\n",
    "parser.add_argument('--future', type=int, default=future, help='Future length')\n",
    "parser.add_argument('--sw', type=int, default=sw, help='Sliding window')\n",
    "parser.add_argument('--sn', type=int, default=sn, help='Sliding number')\n",
    "parser.add_argument('--Columns_to_keep', type=list, default=Columns_to_keep, help='Columns to keep')\n",
    "parser.add_argument('--Columns_to_Predict', type=list, default=Columns_to_Predict, help='Columns to predict')\n",
    "parser.add_argument('--TrfL_Columns', type=list, default=TrfL_Columns, help='Traffic light columns')\n",
    "parser.add_argument('--Nusers', type=int, default=Nusers, help='Number of maneuvers')\n",
    "parser.add_argument('--sos', type=int, default=sos, help='Start of sequence')\n",
    "parser.add_argument('--eos', type=int, default=eos, help='End of sequence')\n",
    "parser.add_argument('--xyidx', type=list, default=xyid, help='X and Y index')\n",
    "parser.add_argument('--Centre', type=list, default=Centre, help='Centre')\n",
    "\n",
    "parser.add_argument('--input_size', type=int, default=input_size, help='Input size')\n",
    "parser.add_argument('--hidden_size', type=int, default=hidden_size, help='Hidden size')\n",
    "parser.add_argument('--num_layersGAT', type=int, default=num_layersGAT, help='Number of layers')\n",
    "parser.add_argument('--num_layersGRU', type=int, default=num_layersGRU, help='Number of layers')\n",
    "parser.add_argument('--output_size', type=int, default=output_size, help='Output size')\n",
    "parser.add_argument('--n_heads', type=int, default=n_heads, help='Number of heads')\n",
    "parser.add_argument('--concat', type=bool, default=concat, help='Concat')\n",
    "parser.add_argument('--dropout', type=float, default=0.001, help='Dropout')\n",
    "parser.add_argument('--leaky_relu_slope', type=float, default=0.2, help='Leaky relu slope')\n",
    "parser.add_argument('--expansion', type=int, default=expansion, help='Expantion')\n",
    "\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=epochs, help='Number of epochs')\n",
    "parser.add_argument('--learning_rate', type=float, default=learning_rate, help='Learning rate')\n",
    "parser.add_argument('--batch_size', type=int, default=batch_size, help='Batch size')\n",
    "parser.add_argument('--patience_limit', type=int, default=patience_limit, help='Patience limit')\n",
    "parser.add_argument('--schd_stepzise', type=int, default=schd_stepzise, help='Scheduler step size')\n",
    "parser.add_argument('--gamma', type=float, default=gamma, help='Scheduler Gamma')\n",
    "\n",
    "\n",
    "parser.add_argument('--only_test', type=bool, default=only_test, help='Only test')\n",
    "parser.add_argument('--generate_data', type=bool, default=generate_data, help='Generate data')\n",
    "parser.add_argument('--loadData', type=bool, default=loadData, help='Load data')\n",
    "parser.add_argument('--Train', type=bool, default=Train, help='Train')\n",
    "parser.add_argument('--test_in_epoch', type=bool, default=test_in_epoch, help='Test in epoch')\n",
    "parser.add_argument('--model_from_scratch', type=bool, default=model_from_scratch, help='Model from scratch')\n",
    "parser.add_argument('--load_the_model', type=bool, default=load_the_model, help='Load the model')\n",
    "parser.add_argument('--Seed', type=bool, default=Seed, help='Seed')\n",
    "parser.add_argument('--device', type=str, default=device, help='device')\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx_graph = nx.from_edgelist(edgindx)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "nx.draw(nx_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the target is in the zone\n",
    "for i in range(len(grth)):\n",
    "    print(zonefinder(grth[i,0]*1024,grth[i,1]*1024, ZoneConf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all of the predicted data\n",
    "for i in range(20, 21):\n",
    "    plt.plot(Predicteddata[-1][i,:,0].to('cpu'), Predicteddata[-1][i,:,1].to('cpu'), 'r')\n",
    "    plt.plot(ego_gt[i][:,0], ego_gt[i][:,1], 'b')\n",
    "\n",
    "# plt.scatter(x,y, marker='x', color='r')\n",
    "# plt.scatter(gyx, gyy, marker='*', color='b')\n",
    "#plt.scatter(groundtruthx[i][:,0], groundtruthx[i][:,1], marker='o', color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import math\n",
    "from convlstm import ConvLSTMCell\n",
    "from utils import *\n",
    "from lstm_traj import *\n",
    "from LSTM import *\n",
    "#from socialconv.model import highwayNet\n",
    "#from torchviz import make_dot\n",
    "# Load the data\n",
    "cwd = os.getcwd()\n",
    "csvpath = os.path.join(cwd,'Processed','Trj20240111T2201.csv')\n",
    "# # Hyperparameters\n",
    "input_size, hidden_size, num_layers, output_size, epochs, learning_rate = 10, 64, 4, 1, 200, 1e-2\n",
    "sequence_length, sw , shift, batch_size = 100, 10, 8, 1\n",
    "sl = sequence_length\n",
    "add_class = True #  Add class by loading the CSV file\n",
    "load_class = not add_class      # load the class from the saved file\n",
    "Train = True # It's for training the model with the prepared data\n",
    "test_in_epoch = True # It's for testing the model in each epoch\n",
    "model_from_scratch = True # It's for creating model from scratch\n",
    "load_the_model = not model_from_scratch # It's for loading model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Create datasets\n",
    "\n",
    "if add_class:\n",
    "    df = loadcsv(csvpath)\n",
    "    dataset, maxdata, mindata = def_class(df, sw, sl, shift, output_size, input_size, normalize = False)\n",
    "    \n",
    "if load_class:\n",
    "    dataset = torch.load(os.path.join(os.getcwd(),'Pickled','datasetTorch.pt'))\n",
    "    normpar = torch.load(os.path.join(os.getcwd(),'Pickled','normpar.pt'))\n",
    "    print('dataset loaded')\n",
    "print(\"Dataset total length is\", len(dataset))\n",
    "\n",
    "# Create train and test sets\n",
    "train_loader, test_loader, val_loader = prep_data(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This part Summarizes the txt files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the log.txt file and append to txt\n",
    "txt = []\n",
    "tmp = \"Hellp\"\n",
    "n=0\n",
    "j = 0\n",
    "with open(os.path.join(os.getcwd(),'logs', 'log-05-07-19-13.txt'), 'r') as file:\n",
    "    logs = file.read()\n",
    "    for log in logs.split('\\n'):\n",
    "        if tmp.startswith('Average') and log.startswith(' Accuracy'):\n",
    "            tmp1 = log\n",
    "            tmp2 = tmp\n",
    "        if log.startswith('Learning'):\n",
    "            j += 1\n",
    "            if j == 1:\n",
    "                txt.append(log)\n",
    "            else:\n",
    "                txt.append(tmp1)\n",
    "                txt.append(tmp2) \n",
    "                txt.append(log)\n",
    "        tmp = log\n",
    "\n",
    "with open(os.path.join(os.getcwd(),'logs', 'Summary of log-05-07-19-13.txt'), 'w') as file:\n",
    "    for log in txt:\n",
    "        file.write(log + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
