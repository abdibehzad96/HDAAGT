{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from utilz.utils import *\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from models.HDAAGT import *\n",
    "from train_test import *\n",
    "from utilz.utils import *\n",
    "import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, we load Configuration file, then generate the data for plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  # of GPUs 4\n",
      "Using cuda:3 device\n",
      "Creating model from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_532248/2606044532.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(config['Load_Model_Path'], map_location=device))\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "with  open(\"/home/abdikhab/HDAAGT/configs/config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "ct = datetime.datetime.now().strftime(\"%m-%d-%H-%M\") # Current time, used for saving the log\n",
    "print(f\"Total  # of GPUs {torch.cuda.device_count()}\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "ZoneConf = Zoneconf(config['Zoneconf_path']) # This is the setting for zone configuration, you can find the image file at 'data/Zone Configuration.jpg'\n",
    "# config non-indipendent parameters\n",
    "config['NFeatures']= len(config[\"Columns_to_keep\"]) # This goes to the data preparation process\n",
    "config[\"input_size\"] = config[\"NFeatures\"] + 9 + 8 # This goes to the model, we add couple of more features to the input\n",
    "config[\"output_size\"] = len(config['xy_indx']) # The number of columns to predict\n",
    "config['device'] = device\n",
    "config['ct'] = ct\n",
    "log_code = f\"Learning rate: {config['learning_rate']}, Hidden size: {config['hidden_size']}, Batch size: {config['batch_size']}\"\n",
    "if config['verbal']:\n",
    "    for arg in config.items(): # Print the arguments to the log file\n",
    "        print(f\"{arg[0]} = {arg[1]}\", ct)\n",
    "config['sos'] = torch.cat((torch.tensor(config['sos']) , torch.zeros(17)), dim=0).repeat(config['Nnodes'], 1).to(device)\n",
    "config['eos'] = torch.cat((torch.tensor(config['eos']), torch.zeros(17)), dim=0).repeat(config['Nnodes'],1).to(device)\n",
    "\n",
    "model = HDAAGT(config).to(device) # Here we load our model's class\n",
    "print(\"Creating model from scratch\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load(config['Load_Model_Path'], map_location=device))\n",
    "learning_rate, hidden_size, num_layers, batch_size, ct= 1e-4, 128, 1, 128, '02-04-13-46'\n",
    "\n",
    "code = f\"LR{int(learning_rate*100000)}_HS{hidden_size}_NL{num_layers}_BS{batch_size} {ct}\"\n",
    "batch = 32\n",
    "dataset = Scenes(config)\n",
    "load = False\n",
    "# load = True\n",
    "if load:\n",
    "    print(\"Loading CSV file ...\", ct)\n",
    "    Traffic_data = loadcsv(config['detection_path'], config['Headers'])\n",
    "    dataset = Scene_Process_plotting(dataset, Traffic_data, config)\n",
    "    dataset.save_class('Processed/video', cmnt = \"video\")\n",
    "    del Traffic_data\n",
    "else:\n",
    "    dataset.load_class('Processed/video', cmnt = \"video\")\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then, we predict the results using the trained model weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1045, 17, 32, 2]) torch.Size([1045, 17, 32, 2])\n",
      "tensor(25167.9941, device='cuda:3') tensor(25167.9941, device='cuda:3')\n",
      "tensor(63.5890, device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "Predicteddata = []\n",
    "ActualTarget = []\n",
    "ActualScene = []\n",
    "model.eval()\n",
    "for Scene, Target, Adj_Mat_Scene in loader:\n",
    "    x, src_mask,adj_mat, target = prep_model_input(Scene, Adj_Mat_Scene,Target, config['sos'], config['eos'], config)\n",
    "    Predicteddata += model(x, src_mask, adj_mat)\n",
    "    ActualTarget += Target\n",
    "    ActualScene += Scene\n",
    "\n",
    "Predicteddata = torch.stack(Predicteddata)\n",
    "ActualTarget = torch.stack(ActualTarget)\n",
    "ActualScene = torch.stack(ActualScene)\n",
    "a = Predicteddata[:,:].softmax(dim=-1)\n",
    "top_values, top_indices = torch.topk(a, k = 3, dim=-1)\n",
    "pred_tar0 = top_indices[:,:,:,:,0]\n",
    "pred_tar = (top_indices*top_values).sum(-1)/top_values.sum(-1)\n",
    "print(pred_tar0.shape, pred_tar.shape)\n",
    "true_loss = F.mse_loss(top_indices[:,1:-1,:,:,0], ActualTarget[:,:,:,:2])\n",
    "lvl2_loss = F.mse_loss(pred_tar0[:,1:-1], ActualTarget[:,:,:,:2])\n",
    "print(true_loss, lvl2_loss)\n",
    "error = torch.sqrt(torch.pow((pred_tar0[:,1:-1]- ActualTarget[:,:,:,:2]),2).sum(-1)).mean()\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_torch(x, window_size):\n",
    "    \"\"\" x: Tensor of shape [batch_size, channels, time] \"\"\"\n",
    "    kernel = torch.ones(1, 1, window_size, device=x.device) / window_size\n",
    "    # Use padding to maintain same output size\n",
    "    x_padded = F.pad(x[1:-1].permute(1,2,0), (window_size - 1, 0), mode='replicate')  # causal padding\n",
    "    xsmoothed = F.conv1d(x_padded[:,:1].to(torch.float), kernel)\n",
    "    ysmoothed = F.conv1d(x_padded[:,1:].to(torch.float), kernel)\n",
    "    return torch.cat((xsmoothed, ysmoothed), dim=1).permute(2,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we create a 2min videos of the scene along the whole dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n",
      "Videos created successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img0 = cv2.imread(\"data/no-car.JPG\")\n",
    "path = \"/home/abdikhab/HDAAGT/Processed/resutls\"\n",
    "icon0 = cv2.imread(\"data/icon.png\", cv2.IMREAD_UNCHANGED)  # Includes alpha\n",
    "trf_light = cv2.imread(\"data/cropTrf.png\", cv2.IMREAD_UNCHANGED)\n",
    "init_pos = [45, 30, -60, 210, -60, 210, 120, 30, 120] # The initial position of the vehicles\n",
    "\n",
    "crop_boxes = [[50,43, 282,293],[50,293,282,540],[50, 540, 282, 791], [50, 791, 282, 1043], [50, 1043, 282, 1295]] # Green, Red, Yellow, LeftGreen, LeftYellow\n",
    "trfl_pos = [[911, 245], [384, 105], [254, 238], [750, 520]]\n",
    "trfl_angle = [-60, 45, 135, -135]\n",
    "def crop_traffic_light(trf_light, crop_boxes, resize):\n",
    "    trf_light_imgs = []\n",
    "    for boxes in crop_boxes:\n",
    "        roi = trf_light[boxes[1]:boxes[3], boxes[0]:boxes[2]]\n",
    "        roi = cv2.resize(roi, resize)\n",
    "        trf_light_imgs.append(roi)\n",
    "    return trf_light_imgs\n",
    "\n",
    "def print_traffic_light(img, trf_light_imgs,trfl_pos, indices):\n",
    "    for n, indx in enumerate(indices):\n",
    "        indx = int(indx)-1\n",
    "        M = cv2.getRotationMatrix2D((25,23), trfl_angle[n], 1.0)\n",
    "        \n",
    "        # Blend icon into background\n",
    "        roi = img[trfl_pos[n][1]:trfl_pos[n][1]+46, trfl_pos[n][0]:trfl_pos[n][0]+50]\n",
    "        icon = cv2.warpAffine(trf_light_imgs[indx], M, (50,46), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0,0))\n",
    "        b, g, r, a = cv2.split(icon)\n",
    "        overlay_color = cv2.merge((b, g, r))\n",
    "        mask = cv2.merge((a, a, a)) / 255.0\n",
    "        roi = roi * (1 - mask) + overlay_color * mask\n",
    "        img[trfl_pos[n][1]:trfl_pos[n][1]+46, trfl_pos[n][0]:trfl_pos[n][0]+50] = roi.astype(np.uint8) # this should be fixed\n",
    "\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path)\n",
    "os.mkdir(path)\n",
    "h0 = 60\n",
    "\n",
    "lower_color = np.array([5, 100, 100, 0])  # lower bound of blue in BGR\n",
    "upper_color = np.array([55, 255, 255, 255])\n",
    "lists = list(range(0, len(ActualScene)))\n",
    "target_color = [(np.random.randint(0,254), np.random.randint(0,254), np.random.randint(0,254)) for i in range(32)]\n",
    "trf_light_imgs = crop_traffic_light(trf_light, crop_boxes, (50, 46))\n",
    "\n",
    "for jj in range(0, 5): # The 5 can be changed to len(ActualScene)//30):\n",
    "    scene = ActualScene[jj]\n",
    "    pred = moving_average_torch(pred_tar0[0,], 3)\n",
    "    video_path = os.path.join(path, f\"pred{jj}.mp4\")\n",
    "    video = cv2.VideoWriter(video_path,cv2.VideoWriter_fourcc(*'mp4v'),2,(1024,1024))\n",
    "    img = img0.copy()\n",
    "    for i in range (1,15): # Scene\n",
    "        tmp_img = img.copy()\n",
    "        for n in range(32):\n",
    "            if scene[i,n,0] > h0 and scene[i-1,n,0] > h0:\n",
    "                h = int(h0 * torch.exp(-scene[i,n,12]))\n",
    "                # Rotation parameters\n",
    "                center = (h // 2, h // 2)\n",
    "                icsize = (2*(h//2), 2*(h//2))\n",
    "                icon = cv2.resize(icon0, icsize)\n",
    "\n",
    "                mask = cv2.inRange(icon, lower_color, upper_color)\n",
    "                # Replace the old color with the new one using the mask\n",
    "                icon[mask > 0, :3] = target_color[n]\n",
    "\n",
    "                angle = 180*torch.atan2(-(scene[i,n,1]-scene[i-1,n,1]), (scene[i,n,0] - scene[i-1,n,0]))/torch.pi\n",
    "                M = cv2.getRotationMatrix2D(center, int(angle), 1.0)\n",
    "                rotated_icon = cv2.warpAffine(icon, M, icsize, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0,0))\n",
    "                # Split rotated icon into BGR and alpha\n",
    "                icon_rgb = rotated_icon[:, :, :3]\n",
    "                alpha = rotated_icon[:, :, 3] / 255.0\n",
    "                # Set position to place icon\n",
    "                # Define ROI on the background\n",
    "            \n",
    "                roi = tmp_img[int(scene[i,n,1]-center[0]):int(scene[i,n,1]+center[0]), int(scene[i,n,0]-center[1]):int(scene[i,n,0]+center[1])]\n",
    "                # Blend icon into background\n",
    "                for c in range(3):\n",
    "                    roi[:, :, c] = roi[:, :, c] * (1 - alpha) + icon_rgb[:, :, c] * alpha\n",
    "\n",
    "\n",
    "                cv2.line(img, (int(scene[i,n,0]), int(scene[i,n,1])),(int(scene[i-1,n,0]), int(scene[i-1,n,1])), color = target_color[n], thickness= 4)\n",
    "                \n",
    "                cv2.putText(tmp_img, \"Recording History\", (412, 580), fontFace=cv2.FONT_HERSHEY_TRIPLEX, thickness= 3, fontScale=1, color=(0, 128, 0))\n",
    "                # Replace the region in the original image\n",
    "                tmp_img[int(scene[i,n,1]-center[0]):int(scene[i,n,1]+center[0]), int(scene[i,n,0]-center[1]):int(scene[i,n,0]+center[1])] = roi\n",
    "        light_stat = torch.max(scene[i,:,3:7], dim=0)[0]\n",
    "        print_traffic_light(tmp_img, trf_light_imgs,trfl_pos, light_stat)\n",
    "        video.write(tmp_img)\n",
    "    for m in lists[jj*30:jj*30+30]:\n",
    "        tar = ActualTarget[m]\n",
    "        pred = pred_tar0[m,] # moving_average_torch(pred_tar0[m,], 3)\n",
    "        img = img0.copy()\n",
    "        tetha0 = torch.zeros(32)\n",
    "        for i in range (1,15): # Target\n",
    "            tmp_img = img.copy()\n",
    "            for n in range(32):\n",
    "                R = torch.sqrt((pred[i,n,1]-512)**2 + (pred[i,n,0]-512)**2)/512\n",
    "                if R < 0.85 and tar[i,n,1] > 50 and tar[i-1,n,1]> 50:\n",
    "                    \n",
    "                    h = int(h0 * torch.exp(-tar[i,n,12]))\n",
    "                    center = (h // 2, h // 2)\n",
    "                    icsize = (2*(h//2), 2*(h//2))\n",
    "                    icon = cv2.resize(icon0, icsize)\n",
    "\n",
    "                    mask = cv2.inRange(icon, lower_color, upper_color)\n",
    "                    icon[mask > 0, :3] = target_color[n]\n",
    "\n",
    "                    angle = 180*torch.atan2(-(scene[i,n,1]-scene[i-1,n,1]), (scene[i,n,0] - scene[i-1,n,0]))/torch.pi\n",
    "\n",
    "                    M = cv2.getRotationMatrix2D(center, int(angle), 1.0)\n",
    "                    rotated_icon = cv2.warpAffine(icon, M, icsize, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0,0))\n",
    "                    # Split rotated icon into BGR and alpha\n",
    "                    icon_rgb = rotated_icon[:, :, :3]\n",
    "                    alpha = rotated_icon[:, :, 3] / 255.0\n",
    "                    roi = tmp_img[int(tar[i,n,1]-center[0]):int(tar[i,n,1]+center[0]), int(tar[i,n,0]-center[1]):int(tar[i,n,0]+center[1])]\n",
    "\n",
    "                    # Blend icon into background\n",
    "                    for c in range(3):\n",
    "                        roi[:, :, c] = roi[:, :, c] * (1 - alpha) + icon_rgb[:, :, c] * alpha\n",
    "\n",
    "                    # Replace the region in the original image\n",
    "                    cv2.line(img, (int(tar[i,n,0]), int(tar[i,n,1])),(int(tar[i-1,n,0]), int(tar[i-1,n,1])), color = target_color[n], thickness= 4)\n",
    "                    cv2.circle(img, (int(pred[i, n, 0]), int(pred[i, n, 1])), 5, color = target_color[n], thickness= -1)\n",
    "                    tmp_img[int(tar[i,n,1]-center[0]):int(tar[i,n,1]+center[0]), int(tar[i,n,0]-center[1]):int(tar[i,n,0]+center[1])] = roi\n",
    "                    cv2.putText(tmp_img, \"Predicting...\", (412, 580),fontFace=cv2.FONT_HERSHEY_TRIPLEX ,thickness= 3, fontScale=1, color =(0, 128, 0) )\n",
    "            light_stat = torch.max(tar[i,:,3:7], dim=0)[0]\n",
    "            print_traffic_light(tmp_img, trf_light_imgs,trfl_pos, light_stat)      \n",
    "            video.write(tmp_img)\n",
    "    video.release()\n",
    "    print(\"Videos created successfully\")\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Model details*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "HDAAGT                                        [1, 17, 32, 2, 1024]      --\n",
       "├─Encoder_DAAG: 1-1                           [32, 17, 384]             --\n",
       "│    └─Positional_Encoding_Layer: 2-1         [32, 17, 384]             296,064\n",
       "│    │    └─ModuleList: 3-1                   --                        262,144\n",
       "│    │    └─DAAG_Layer: 3-2                   [1, 17, 32, 256]          211,456\n",
       "│    │    └─TemporalConv: 3-3                 [32, 17, 128]             20,892\n",
       "│    │    └─MultiheadAttention: 3-4           [32, 17, 384]             591,360\n",
       "│    │    └─Dropout: 3-5                      [32, 17, 384]             --\n",
       "│    └─Traffic_Encoding_Layer: 2-2            [32, 17, 384]             1,152\n",
       "│    │    └─ModuleList: 3-6                   --                        1,248\n",
       "│    │    └─LayerNorm: 3-7                    [1, 17, 32, 17]           34\n",
       "│    │    └─Linear: 3-8                       [1, 17, 32, 128]          2,304\n",
       "│    │    └─Linear: 3-9                       [1, 17, 32, 128]          16,512\n",
       "│    │    └─Linear: 3-10                      [1, 17, 32, 128]          16,512\n",
       "│    │    └─MultiheadAttention: 3-11          [32, 17, 384]             591,360\n",
       "│    │    └─Dropout: 3-12                     [32, 17, 384]             --\n",
       "│    │    └─FeedForwardNetwork: 3-13          [32, 17, 384]             295,680\n",
       "│    │    └─Dropout: 3-14                     [32, 17, 384]             --\n",
       "│    └─Mixed_Attention_Layer: 2-3             [32, 17, 384]             768\n",
       "│    │    └─MultiheadAttention: 3-15          [32, 17, 384]             591,360\n",
       "│    │    └─FeedForwardNetwork: 3-16          [32, 17, 384]             295,680\n",
       "│    │    └─Dropout: 3-17                     [32, 17, 384]             --\n",
       "├─Projection: 1-2                             [1, 17, 32, 2, 1024]      --\n",
       "│    └─LayerNorm: 2-4                         [32, 17, 384]             768\n",
       "│    └─Linear: 2-5                            [32, 17, 2048]            788,480\n",
       "===============================================================================================\n",
       "Total params: 3,983,774\n",
       "Trainable params: 3,983,774\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 60.54\n",
       "===============================================================================================\n",
       "Input size (MB): 0.12\n",
       "Forward/backward pass size (MB): 30.71\n",
       "Params size (MB): 7.64\n",
       "Estimated Total Size (MB): 38.48\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary  # more flexible than torchsummary\n",
    "learning_rate, hidden_size, num_layers, batch_size, ct= 1e-4, 128, 1, 128, '02-04-13-46'\n",
    "code = f\"LR{int(learning_rate*100000)}_HS{hidden_size}_NL{num_layers}_BS{batch_size} {ct}\"\n",
    "# Predicteddata  = torch.load(os.path.join(os.getcwd(),'Pickled', code + 'Predicteddata.pt'), weights_only=True)\n",
    "ActualTarget = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Target.pt'), weights_only=True)\n",
    "ActualScene = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Scene.pt'), weights_only=True)\n",
    "Adj_Mat_Scene = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Adj_Mat_Scene.pt'), weights_only=True)\n",
    "# Adj_Mat_Target = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Adj_Mat_Target.pt'), weights_only=True)\n",
    "# test_loss = torch.load(os.path.join(os.getcwd(),'Pickled', code + 'test_losses.pt'), weights_only=True)\n",
    "epoch_losses = torch.load(os.path.join(cwd,'Pickled', f'epoch_losses.pt'), weights_only=True)\n",
    "\n",
    "# Dummy input example (adjust shapes accordingly)\n",
    "x = attach_sos_eos(ActualScene[:1], config['sos'], config['eos'])\n",
    "adj_mat = torch.cat((torch.ones_like(Adj_Mat_Scene[:1,:1]), Adj_Mat_Scene[:1], torch.ones_like(Adj_Mat_Scene[:1,:1])), dim=1)\n",
    "src_mask = create_src_mask(x)\n",
    "# custom_summary(model, x, src_mask, adj_mat)\n",
    "summary(model, input_size=[x.size(), src_mask.size(), adj_mat.size()], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Model diagram illustration*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/HDAAGT_graph.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "model.eval()\n",
    "output = model(x, src_mask, adj_mat)\n",
    "\n",
    "# Create graph\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('models/HDAAGT_graph', cleanup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The rest is working but not maintained***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ct' must be selected manually, it marks the unique model names, which contains date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate, hidden_size, num_layers, batch_size, ct= 1e-4, 128, 1, 128, '02-04-13-46'\n",
    "code = f\"LR{int(learning_rate*100000)}_HS{hidden_size}_NL{num_layers}_BS{batch_size} {ct}\"\n",
    "Predicteddata  = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Predicteddata.pt'), weights_only=True)\n",
    "ActualTarget = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Target.pt'), weights_only=True)\n",
    "ActualScene = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Scene.pt'), weights_only=True)\n",
    "Adj_Mat_Scene = torch.load(os.path.join(os.getcwd(),'Pickled', 'Test', 'Adj_Mat_Scene.pt'), weights_only=True)\n",
    "epoch_losses = torch.load(os.path.join(cwd,'Pickled', f'epoch_losses{ct``}.pt'), weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_color = [\n",
    "    (0, 0, 0),        # Black\n",
    "    (0, 0, 255),      # Red\n",
    "    (0, 255, 0),      # Green\n",
    "    (255, 0, 0),      # Blue\n",
    "    (0, 255, 255),    # Yellow\n",
    "    (255, 255, 0),    # Cyan\n",
    "    (255, 0, 255),    # Magenta\n",
    "    (128, 0, 128),    # Purple\n",
    "    (0, 165, 255),    # Orange\n",
    "    (130, 0, 75),     # Indigo\n",
    "    (128, 128, 0),    # Teal\n",
    "    (0, 0, 128),      # Maroon\n",
    "    (128, 0, 0),       # Navy\n",
    "    (128, 128, 128),  # Gray\n",
    "    (192, 192, 192),  # Silver\n",
    "    (255, 255, 255),   # White\n",
    "    (255, 165, 0),     # Brown\n",
    "    (0, 128, 0),       # Olive\n",
    "    (0, 128, 128),     # Aqua\n",
    "    (128, 0, 128),     # Fuchsia\n",
    "    (128, 128, 0),     # Lime\n",
    "# Some more colors\n",
    "    (50, 50, 50),      # Dark Gray\n",
    "    (60, 120, 120),    # Sea Green\n",
    "    (120, 30, 60),     # Crimson\n",
    "    (100, 20, 120),    # Violet\n",
    "    (10, 32, 64),       # Dark Blue\n",
    "    (64,10, 32),        # Dark Red\n",
    "    (10, 64, 32),       # Dark Green\n",
    "    (16, 32, 64),       # Dark Aqua\n",
    "    (32, 64, 16),       # Dark Lime\n",
    "    (64, 32, 16),       # Dark Orange\n",
    "    (64, 16, 32),       # Dark Pink\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import re\n",
    "from shapely.geometry import Point, Polygon\n",
    "def Zoneconf1():\n",
    "    ZoneConf = []\n",
    "    with open('utilz/ZoneConf.yaml') as file:\n",
    "        ZonesYML = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        #convert the string values to float\n",
    "        for _, v in ZonesYML.items():\n",
    "            lst = []\n",
    "            for _, p  in v.items():    \n",
    "                for x in p[0]:\n",
    "                    b = re.split(r'[,()]',p[0][x])\n",
    "                    lst.append((float(b[1]), float(b[2])))\n",
    "            ZoneConf.append(lst)\n",
    "    return ZoneConf\n",
    "\n",
    "def zonefinder(BB, Zones):\n",
    "    for i, zone in enumerate(Zones):\n",
    "        Poly = Polygon(zone)\n",
    "        if Poly.contains(Point(BB[0], BB[1])):\n",
    "            return i\n",
    "    return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoneconf = Zoneconf1()\n",
    "zones = torch.empty(0,2)\n",
    "zonesw0 = torch.empty(0,2)\n",
    "err = 0\n",
    "for m in range(440):\n",
    "    tar = target[m,-1].to('cpu')\n",
    "    pred = pred_tar[m,-4:-1].mean(0).to('cpu')\n",
    "    for i in range(32):\n",
    "        zt = zonefinder(tar[i],zoneconf)\n",
    "        zp = zonefinder(pred[i], zoneconf)\n",
    "        if zp == 100 and zt != 100:\n",
    "            # print(pred[i], tar[i], zp, zt)\n",
    "            err +=1\n",
    "        else:\n",
    "            zonesw0 = torch.cat((zonesw0,torch.tensor([[zp,zt]])), dim=0)\n",
    "            if zt !=100:\n",
    "                zones = torch.cat((zones,torch.tensor([[zp,zt]])), dim=0)\n",
    "\n",
    "print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (zones[:,0]==zones[:,1]).sum()\n",
    "b = zones.size(0)\n",
    "print(\"Accuracy without zone 100 is \",a, b, a/b )\n",
    "a = (zonesw0[:,0]==zonesw0[:,1]).sum()\n",
    "b = zonesw0.size(0)\n",
    "print(\"Accuracy with zone 100 is \",a, b, a/b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(zones[:,1], zones[:,0])\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm )\n",
    "disp = disp.plot(cmap=plt.cm.YlGn,values_format='g')\n",
    "# disp.plot()\n",
    "plt.xlabel('Predicted Zones')\n",
    "plt.ylabel('True Zones')\n",
    "plt.title('Confusion Matrix')\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)\n",
    "disp = disp.plot(cmap=plt.cm.YlGnBu,values_format='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch = 32\n",
    "Predicteddata = []\n",
    "for n in range(0, ActualScene.size(0), batch):\n",
    "    x, src_mask,adj_mat, target = prep_model_input(ActualScene[n:n+batch], Adj_Mat_Scene[n:n+batch], ActualTarget[n:n+batch], config['sos'], config['eos'], config)\n",
    "    Predicteddata += model(x, src_mask, adj_mat)\n",
    "\n",
    "Predicteddata = torch.stack(Predicteddata)\n",
    "a = Predicteddata[:,:].softmax(dim=-1)\n",
    "top_values, top_indices = torch.topk(a, k = 5, dim=-1)\n",
    "pred_tar0 = top_indices[:,:,:,:,0]\n",
    "pred_tar = (top_indices*top_values).sum(-1)/top_values.sum(-1)\n",
    "print(pred_tar0.shape, pred_tar.shape)\n",
    "true_loss = F.mse_loss(top_indices[:,1:-1,:,:,0], ActualTarget[:,:,:,1:3])\n",
    "lvl2_loss = F.mse_loss(pred_tar0[:,1:-1], ActualTarget[:,:,:,1:3])\n",
    "print(true_loss, lvl2_loss)\n",
    "error = torch.sqrt(torch.pow((pred_tar0[:,1:-1]- ActualTarget[:,:,:,1:3]),2).sum(-1)).mean()\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img0 = cv2.imread(\"/home/abdikhab/Traj_Pred/RawData/no-car.JPG\")\n",
    "path = \"/home/abdikhab/New_Idea_Traj_Pred/results\"\n",
    "for m in range(440):\n",
    "    img = img0.copy()\n",
    "    scene = scene[m]\n",
    "    tar = target[m,-1]\n",
    "    pred = pred_tar0[m,-2]\n",
    "    for i in range(32):\n",
    "        \n",
    "        egox, egoy = scene[:, i, 0], scene[:,i, 1]\n",
    "        targetx, targety = tar[i, 0], tar[i, 1]\n",
    "        # if dir != 0:\n",
    "        for n in range(int(egox.size(0)-1)):\n",
    "            if egox[n] > 10 and egox[n+1] > 10:\n",
    "                cv2.line(img, (int(egox[n]), int(egoy[n])),(int(egox[n+1]), int(egoy[n+1])), color = rand_color[i], thickness= 4)\n",
    "                # cv2.circle(img, (int(egox[n]), int(egoy[n])), 5, rand_color[i], -1)\n",
    "        cv2.circle(img, (int(tar[i, 0]), int(tar[i, 1])), 10, rand_color[i], -1)\n",
    "        cv2.rectangle(img, (int(pred[i, 0]), int(pred[i, 1])), (int(pred[i, 0]+15), int(pred[i, 1]+15)), rand_color[i], -1)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(path, f\"pred{m}.jpg\"), img)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "print(\"Images created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "'aliceblue':            '#F0F8FF',\n",
    "'antiquewhite':         '#FAEBD7',\n",
    "'aqua':                 '#00FFFF',\n",
    "'aquamarine':           '#7FFFD4',\n",
    "'azure':                '#F0FFFF',\n",
    "'beige':                '#F5F5DC',\n",
    "'bisque':               '#FFE4C4',\n",
    "'black':                '#000000',\n",
    "'blanchedalmond':       '#FFEBCD',\n",
    "'blue':                 '#0000FF',\n",
    "'blueviolet':           '#8A2BE2',\n",
    "'brown':                '#A52A2A',\n",
    "'burlywood':            '#DEB887',\n",
    "'cadetblue':            '#5F9EA0',\n",
    "'chartreuse':           '#7FFF00',\n",
    "'chocolate':            '#D2691E',\n",
    "'coral':                '#FF7F50',\n",
    "'cornflowerblue':       '#6495ED',\n",
    "'cornsilk':             '#FFF8DC',\n",
    "'crimson':              '#DC143C',\n",
    "'cyan':                 '#00FFFF',\n",
    "'darkblue':             '#00008B',\n",
    "'darkcyan':             '#008B8B',\n",
    "'darkgoldenrod':        '#B8860B',\n",
    "'darkgray':             '#A9A9A9',\n",
    "'darkgreen':            '#006400',\n",
    "'darkkhaki':            '#BDB76B',\n",
    "'darkmagenta':          '#8B008B',\n",
    "'darkolivegreen':       '#556B2F',\n",
    "'darkorange':           '#FF8C00',\n",
    "'darkorchid':           '#9932CC',\n",
    "'darkred':              '#8B0000',\n",
    "'darksalmon':           '#E9967A',\n",
    "'darkseagreen':         '#8FBC8F',\n",
    "'darkslateblue':        '#483D8B',\n",
    "'darkslategray':        '#2F4F4F',\n",
    "'darkturquoise':        '#00CED1',\n",
    "'darkviolet':           '#9400D3',\n",
    "'deeppink':             '#FF1493',\n",
    "'deepskyblue':          '#00BFFF',\n",
    "'dimgray':              '#696969',\n",
    "'dodgerblue':           '#1E90FF',\n",
    "'firebrick':            '#B22222',\n",
    "'floralwhite':          '#FFFAF0',\n",
    "'forestgreen':          '#228B22',\n",
    "'fuchsia':              '#FF00FF',\n",
    "'gainsboro':            '#DCDCDC',\n",
    "'ghostwhite':           '#F8F8FF',\n",
    "'gold':                 '#FFD700',\n",
    "'goldenrod':            '#DAA520',\n",
    "'gray':                 '#808080',\n",
    "'green':                '#008000',\n",
    "'greenyellow':          '#ADFF2F',\n",
    "'honeydew':             '#F0FFF0',\n",
    "'hotpink':              '#FF69B4',\n",
    "'indianred':            '#CD5C5C',\n",
    "'indigo':               '#4B0082',\n",
    "'ivory':                '#FFFFF0',\n",
    "'khaki':                '#F0E68C',\n",
    "'lavender':             '#E6E6FA',\n",
    "'lavenderblush':        '#FFF0F5',\n",
    "'lawngreen':            '#7CFC00',\n",
    "'lemonchiffon':         '#FFFACD',\n",
    "'lightblue':            '#ADD8E6',\n",
    "'lightcoral':           '#F08080',\n",
    "'lightcyan':            '#E0FFFF',\n",
    "'lightgoldenrodyellow': '#FAFAD2',\n",
    "'lightgreen':           '#90EE90',\n",
    "'lightgray':            '#D3D3D3',\n",
    "'lightpink':            '#FFB6C1',\n",
    "'lightsalmon':          '#FFA07A',\n",
    "'lightseagreen':        '#20B2AA',\n",
    "'lightskyblue':         '#87CEFA',\n",
    "'lightslategray':       '#778899',\n",
    "'lightsteelblue':       '#B0C4DE',\n",
    "'lightyellow':          '#FFFFE0',\n",
    "'lime':                 '#00FF00',\n",
    "'limegreen':            '#32CD32',\n",
    "'linen':                '#FAF0E6',\n",
    "'magenta':              '#FF00FF',\n",
    "'maroon':               '#800000',\n",
    "'mediumaquamarine':     '#66CDAA',\n",
    "'mediumblue':           '#0000CD',\n",
    "'mediumorchid':         '#BA55D3',\n",
    "'mediumpurple':         '#9370DB',\n",
    "'mediumseagreen':       '#3CB371',\n",
    "'mediumslateblue':      '#7B68EE',\n",
    "'mediumspringgreen':    '#00FA9A',\n",
    "'mediumturquoise':      '#48D1CC',\n",
    "'mediumvioletred':      '#C71585',\n",
    "'midnightblue':         '#191970',\n",
    "'mintcream':            '#F5FFFA',\n",
    "'mistyrose':            '#FFE4E1',\n",
    "'moccasin':             '#FFE4B5',\n",
    "'navajowhite':          '#FFDEAD',\n",
    "'navy':                 '#000080',\n",
    "'oldlace':              '#FDF5E6',\n",
    "'olive':                '#808000',\n",
    "'olivedrab':            '#6B8E23',\n",
    "'orange':               '#FFA500',\n",
    "'orangered':            '#FF4500',\n",
    "'orchid':               '#DA70D6',\n",
    "'palegoldenrod':        '#EEE8AA',\n",
    "'palegreen':            '#98FB98',\n",
    "'paleturquoise':        '#AFEEEE',\n",
    "'palevioletred':        '#DB7093',\n",
    "'papayawhip':           '#FFEFD5',\n",
    "'peachpuff':            '#FFDAB9',\n",
    "'peru':                 '#CD853F',\n",
    "'pink':                 '#FFC0CB',\n",
    "'plum':                 '#DDA0DD',\n",
    "'powderblue':           '#B0E0E6',\n",
    "'purple':               '#800080',\n",
    "'red':                  '#FF0000',\n",
    "'rosybrown':            '#BC8F8F',\n",
    "'royalblue':            '#4169E1',\n",
    "'saddlebrown':          '#8B4513',\n",
    "'salmon':               '#FA8072',\n",
    "'sandybrown':           '#FAA460',\n",
    "'seagreen':             '#2E8B57',\n",
    "'seashell':             '#FFF5EE',\n",
    "'sienna':               '#A0522D',\n",
    "'silver':               '#C0C0C0',\n",
    "'skyblue':              '#87CEEB',\n",
    "'slateblue':            '#6A5ACD',\n",
    "'slategray':            '#708090',\n",
    "'snow':                 '#FFFAFA',\n",
    "'springgreen':          '#00FF7F',\n",
    "'steelblue':            '#4682B4',\n",
    "'tan':                  '#D2B48C',\n",
    "'teal':                 '#008080',\n",
    "'thistle':              '#D8BFD8',\n",
    "'tomato':               '#FF6347',\n",
    "'turquoise':            '#40E0D0',\n",
    "'violet':               '#EE82EE',\n",
    "'wheat':                '#F5DEB3',\n",
    "'white':                '#FFFFFF',\n",
    "'whitesmoke':           '#F5F5F5',\n",
    "'yellow':               '#FFFF00',\n",
    "'yellowgreen':          '#9ACD32'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
